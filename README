DE-DUPLICATION ESTIMATOR

This project is the pre-migration aid for compressed/de-duplicated
filesystems. It helps to know the possible savings having de-duplication
and/or compression enabled on the filesystem before actually committing
to this decision.

The tools walks the real filesystem and emulates per-block compression
and de-duplication, printing out the compressibility info. This can
also be used as the information entropy estimate.

Quick-start:
--------------------------------------------------------------------

Build for source:
 $ mvn clean install
 $ java -jar target/dedup.jar <path>

Binary builds are available here:
 https://builds.shipilev.net/dedup-estimator/

Sample output:
--------------------------------------------------------------------

/home/shade/trunks/, using 4096-byte blocks
Running at 1670.28 MB/sec (5872.08 GB/hour), 2566030/2566030 files, 61178/61178 MB, ETA: 0s
COMPRESS:       1.278x increase, 61,178 MB --(block-compress)--> 47,878 MB
DEDUP:          1.318x increase, 61,178 MB ------(dedup)-------> 46,406 MB
DEDUP+COMPRESS: 1.613x increase, 61,178 MB ------(dedup)-------> 46,406 MB --(block-compress)--> 37,938 MB

The numbers above are from file cache. Actual performance might be lower,
especially when slower disks are used.

Settings to play with:
--------------------------------------------------------------------

 -Dthreads = # (default is #numCPU)
    Number of processing threads.

 -DblockSize = # (default is 4096)
    Target filesystem block size.

 -Dhash = # (default is "SHA-256")
    Hash to use for deduplication

 -Dstorage = # (default is "inmemory")
    Hash storage implementation. Bundled implementations:
      - inmemory: uses ConcurrentHashMap to store on heap
      - berkeley: uses on-disk BerkeleyDB
      - h2:       uses on-disk H2
      - derby:    uses on-disk Apache Derby

Caveats:
--------------------------------------------------------------------

* Default mode uses in-memory hash storage, which can OOM on large
  enough datasets. Consider using off-heap storage for large FSes.
  The rule of thumb: with 4K blocks, you will need ~1 GB of heap space
  per 15 GB of dedup-ed FS size.
