DE-DUPLICATION ESTIMATOR

This project is the pre-migration aid for compressed/de-duplicated
filesystems. It helps to know the possible savings having de-duplication
and/or compression enabled on the filesystem before actually committing
to this decision.

The tools walks the real filesystem and emulates per-block compression
and de-duplication, printing out the compressibility info. This can
also be used as the information entropy estimate.

Quick-start:
--------------------------------------------------------------------

Build for source:
 $ mvn clean install
 $ java -jar target/dedup.jar <path>

Sample output:
--------------------------------------------------------------------

/home/buildbot/worker/, using 128 KB blocks
Running at 962.73 MB/sec (3384.62 GB/hour), 1625177/1625176 dirs, 7725059/7725059 files, 123810/123810 MB
COMPRESS:       1.858x increase, 123,810 MB --(block-compress)--> 66,642 MB
DEDUP:          1.806x increase, 123,810 MB ------(dedup)-------> 68,569 MB
DEDUP+COMPRESS: 3.890x increase, 123,810 MB ------(dedup)-------> 68,569 MB --(block-compress)--> 31,831 MB

The numbers above are from the decently fast SSD and busy build node.
Actual performance depends on what bottlenecks first: the CPU compressing/hashing
things, or the I/O that feeds it.

Settings to play with:
--------------------------------------------------------------------

 -Dthreads = # (default is #numCPU)
    Number of processing threads.

 -DblockSize = # (default is 128)
    Target filesystem block size, KB

 -Dhash = # (default is "SHA-256")
    Hash to use for deduplication

 -Dstorage = # (default is "inmemory")
    Hash storage implementation. Bundled implementations:
      - inmemory: uses ConcurrentHashMap to store on heap
      - berkeley: uses on-disk BerkeleyDB
      - h2:       uses on-disk H2
      - derby:    uses on-disk Apache Derby

Caveats:
--------------------------------------------------------------------

* Default mode uses in-memory hash storage, which can OOM on large
  enough datasets. Consider using off-heap storage for large FSes.
  The rule of thumb: with 128K blocks, you will need ~1 GB of Java heap
  space per 100 GB of dedup-ed FS size. Set larger -Xmx, if needed.
